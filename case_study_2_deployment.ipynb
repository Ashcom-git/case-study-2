{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "case study 2 deployment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wX1CXnMwMRh"
      },
      "source": [
        "!pip -q install streamlit\n",
        "!pip -q install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Ev83Epwko7"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrUwee8vwoMC",
        "outputId": "47cb2098-e1a6-422e-8376-652530f43918"
      },
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.cluster import KMeans\n",
        "from tensorflow.keras.layers import Layer, Embedding, Input, Dense, Activation\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import Model,regularizers, initializers, constraints\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow.keras.backend as K\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "stop = pickle.load(open(\"/content/drive/MyDrive/Aspect/Model_weights_17th_August/stop_words.pkl\", \"rb\"))\n",
        "lemma = pickle.load(open(\"/content/drive/MyDrive/Aspect/Model_weights_17th_August/lemma.pkl\", \"rb\"))\n",
        "\n",
        "batch_size = 512\n",
        "emb_dim = 200\n",
        "aspect_size = 14\n",
        "epochs = 15\n",
        "neg_size = 20\n",
        "seed = 1234\n",
        "domain = 'restaurant'\n",
        "lambda_ = 0.1\n",
        "emb_path = '/content/drive/MyDrive/Aspect/Model_weights_17th_August/w2v_embedding'\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "  text_token = CountVectorizer().build_tokenizer()(text.lower())\n",
        "  text_token = [lemma.lemmatize(i) for i in text_token if i not in stop]\n",
        "  if len(text_token)>0:\n",
        "    return ' '.join(text_token)+'\\n'\n",
        "  return '\\n'  \n",
        "\n",
        "def complete_preprocess(list_):\n",
        "\n",
        "  processed_lst = []\n",
        "  for i in list_:\n",
        "    pre = preprocess(i)\n",
        "    processed_lst.append(pre) \n",
        " \n",
        "  return processed_lst   \n",
        "\n",
        "title = st.text_input('Enter review')\n",
        "if title:\n",
        "  test_2 = complete_preprocess([title]) \n",
        "\n",
        "  def split_list(list_):\n",
        "    \n",
        "    split_ = []\n",
        "    for i in list_:\n",
        "      split_.append(i.split())\n",
        "    return split_   \n",
        "\n",
        "  sp_test = split_list(test_2)\n",
        "\n",
        "  def read_dataset( phase, vocab):\n",
        "\n",
        "      num_hit, unk_hit, total = 0, 0, 0\n",
        "      data_lst = []\n",
        "\n",
        "      for line in phase:\n",
        "          if len(line) > 0:\n",
        "            indices = []\n",
        "            for word in line:\n",
        "                if word.replace('.','',1).isdigit():\n",
        "                    indices.append(vocab['<num>'])\n",
        "                    num_hit += 1\n",
        "                elif word in vocab:\n",
        "                    indices.append(vocab[word])\n",
        "                else:\n",
        "                    indices.append(vocab['<unk>'])\n",
        "                    unk_hit += 1\n",
        "                total += 1\n",
        "            data_lst.append(indices)\n",
        "\n",
        "      if len(phase)>20000:\n",
        "        set_ = 'train'\n",
        "      else:\n",
        "        set_ = 'test'  \n",
        "      return data_lst\n",
        "\n",
        "  vocab_dict = pickle.load(open(\"/content/drive/MyDrive/Aspect/Model_weights_17th_August/vocab_dict.pkl\", \"rb\"))\n",
        "  test_sequence = read_dataset(sp_test,vocab_dict)\n",
        "  test_sequence = pad_sequences(test_sequence, maxlen=156)\n",
        "\n",
        "  class Average(Layer):\n",
        "    \n",
        "    def __init__(self,**kwargs):\n",
        "      self.supports_masking = True\n",
        "      super(Average,self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Average, self).get_config()\n",
        "        return config  \n",
        "\n",
        "    def call(self, x, mask):\n",
        "      \n",
        "      mask = tf.cast(mask,tf.float32)\n",
        "      mask = K.expand_dims(mask)\n",
        "      x *= mask\n",
        "      return tf.reduce_sum(x, axis=-2) / tf.reduce_sum(mask, axis=-2)\n",
        "\n",
        "  class Attention(Layer):\n",
        "\n",
        "    def __init__(self, bias=True, name=None):\n",
        "      self.supports_masking = True\n",
        "      self.init = initializers.get('glorot_uniform')\n",
        "      self.bias = bias\n",
        "      super(Attention,self).__init__(name=name)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Attention, self).get_config()\n",
        "        config.update({\"bias\": self.bias})\n",
        "        return config    \n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "      self.M = self.add_weight(shape=(input_shape[0][-1], input_shape[1][-1]),initializer=self.init,name='{}_W'.format(self.name))\n",
        "      if self.bias:\n",
        "          self.b = self.add_weight(shape=(1,),initializer='zero',name='{}_b'.format(self.name))\n",
        "      self.built=True\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "          x = input_tensor[0]\n",
        "          y = input_tensor[1]\n",
        "          mask = mask[0]\n",
        "          tmp = tf.expand_dims(tf.matmul(y, self.M), axis=-1)\n",
        "          d_i = tf.matmul(x, tmp)+self.b\n",
        "          a_i = tf.nn.softmax(d_i, axis=1)\n",
        "          a_i = tf.squeeze(a_i, axis=-1)\n",
        "          return a_i\n",
        "\n",
        "  def ortho_reg(weight_matrix):\n",
        "      w_n = weight_matrix/tf.cast(K.epsilon()+tf.math.sqrt(tf.reduce_sum(tf.math.square(weight_matrix), axis=-1, keepdims=True)),tf.float32)\n",
        "      reg = tf.reduce_sum(tf.math.square(K.dot(w_n, tf.transpose(w_n)) - tf.eye(w_n.shape[0])))\n",
        "      return lambda_*reg\n",
        "\n",
        "  class WeightedSum(Layer):\n",
        "    \n",
        "    def __init__(self,name=None):\n",
        "      self.supports_masking = True\n",
        "      super(WeightedSum,self).__init__(name=name)\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super(WeightedSum, self).get_config()\n",
        "      return config   \n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        x = input_tensor[0]\n",
        "        a = K.expand_dims(input_tensor[1])\n",
        "        weighted_input = x*a\n",
        "        return tf.reduce_sum(weighted_input, axis=1)   \n",
        "\n",
        "  class HingeLoss(Layer):\n",
        "\n",
        "    def __init__(self,name=None):\n",
        "      super(HingeLoss,self).__init__(name=name) \n",
        "\n",
        "    def get_config(self):\n",
        "      config = super(HingeLoss, self).get_config()\n",
        "      return config  \n",
        "\n",
        "    def call(self,outputs):\n",
        "      zn = outputs[0]\n",
        "      zp = outputs[1]\n",
        "      rs = outputs[2]\n",
        "      zn = K.l2_normalize(zn, axis=-1)\n",
        "      zp = K.l2_normalize(zp, axis=-1)\n",
        "      rs = K.l2_normalize(rs, axis=-1)\n",
        "\n",
        "      steps = zn.shape[1]\n",
        "      pos = tf.reduce_sum(zp*rs, axis=-1, keepdims=True)\n",
        "      pos = K.repeat_elements(pos, steps, axis=1)\n",
        "      rs = K.expand_dims(rs, axis=-2)\n",
        "      rs = K.repeat_elements(rs, steps, axis=1)\n",
        "      neg = tf.reduce_sum(zn*rs, axis=-1)\n",
        "\n",
        "      loss = tf.cast(tf.reduce_sum(K.maximum(tf.cast(0,tf.float32), (tf.cast(1,tf.float32) - pos + neg)), axis=-1, keepdims=True), tf.float32)\n",
        "      return loss\n",
        "\n",
        "  class WeightedAspectEmb(Layer):\n",
        "      def __init__(self, input_dim, output_dim,init='uniform',name=None,W_regularizer=None):\n",
        "\n",
        "          self.input_dim = input_dim\n",
        "          self.output_dim = output_dim\n",
        "          self.init = initializers.get(init)\n",
        "          self.W_regularizer = regularizers.get(W_regularizer)\n",
        "          super(WeightedAspectEmb,self).__init__(name=name)\n",
        "\n",
        "      def get_config(self):\n",
        "        config = super(WeightedAspectEmb, self).get_config()\n",
        "        config.update({\"input_dim\": self.input_dim, 'output_dim':self.output_dim, 'init':self.init})\n",
        "        return config      \n",
        "\n",
        "      def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=[self.input_dim, self.output_dim],regularizer=self.W_regularizer,initializer='uniform',name='{}_W'.format(self.name))  \n",
        "        self.built = True\n",
        "\n",
        "      def call(self, x, mask=None):\n",
        "          return K.dot(x, self.W)    \n",
        "\n",
        "\n",
        "  pos_input = Input(shape=(156), dtype='int32', name='sentence_input')\n",
        "  neg_input = Input(shape=(neg_size, 156), dtype='int32', name='neg_input')\n",
        "  embedding = Embedding(len(vocab_dict),emb_dim,  mask_zero=True, name='word_embedding', trainable=False)\n",
        "  pos_emb = embedding(pos_input)\n",
        "  ys = Average(name='ys')(pos_emb)\n",
        "  attn = Attention(emb_dim,name='attention')([pos_emb,ys])\n",
        "  zp = WeightedSum(name='zp')([pos_emb,attn])\n",
        "  neg_emb = embedding(neg_input)\n",
        "  zn = Average(name='zn')(neg_emb)\n",
        "  pt = Dense(aspect_size, name='pt_previous')(zp)\n",
        "  pt = Activation('softmax', name='pt')(pt)\n",
        "  rs = Dense(emb_dim, name='asp_emb',kernel_regularizer=ortho_reg,use_bias=False)(pt)\n",
        "  loss = HingeLoss(name='hinge_loss')([zn, zp, rs])\n",
        "\n",
        "  new_model = Model(inputs=[pos_input, neg_input], outputs=loss)         \n",
        "  new_model.load_weights('/content/drive/MyDrive/Aspect/Model_weights_17th_August/weights.h5')\n",
        "\n",
        "  cluster_map_absa = {0:'Miscellaneous\\n',1:'Food\\n',2:'Price\\n',3:'Miscellaneous\\n',4:'Anecdotes\\n',5:'Miscellaneous\\n',6:'Food\\n',\n",
        "                      7:'Food\\n',8:'Miscellaneous\\n',9:'Miscellaneous\\n',10:'Ambience\\n',11:'Staff\\n',12:'Food\\n',13:'Food\\n'}\n",
        "\n",
        "  test_fn = Model(inputs=new_model.get_layer('sentence_input').input,outputs=new_model.get_layer('pt').output)\n",
        "  aspect_probs = test_fn(test_sequence)\n",
        "\n",
        "  label_ids = np.argsort(aspect_probs, axis=1)[:,-1]\n",
        "  predict_labels_absa = [cluster_map_absa[label_id][:-1] for label_id in label_ids][0]     \n",
        "  st.write('Aspect: {}'.format(predict_labels_absa))    \n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pd9Hvem0ttD",
        "outputId": "a25d1db9-82e5-415d-a90d-dcd38390ca96"
      },
      "source": [
        "!ngrok authtoken 1s9EWYEkYGU9J6chwq2Z0Hucw5R_2iBbA53YyhmegQY3b67nm"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0CWdxer0w_r",
        "outputId": "86ce8513-46a1-43a0-dd11-546e937bcfe1"
      },
      "source": [
        "public_url = ngrok.connect(port='80')\n",
        "print (public_url)\n",
        "!streamlit run --server.port 80 app.py >/dev/null "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"http://7909-35-245-178-193.ngrok.io\" -> \"http://localhost:80\"\n",
            "2021-08-26 11:47:38.198 'pattern' package not found; tag filters are not available for English\n",
            "2021-08-26 11:48:02.693641: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-08-26 11:48:02.693695: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3bfaf3d6f2d0): /proc/driver/nvidia/version does not exist\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}